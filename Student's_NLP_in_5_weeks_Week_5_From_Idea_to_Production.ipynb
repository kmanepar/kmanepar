{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmanepar/kmanepar/blob/main/Student's_NLP_in_5_weeks_Week_5_From_Idea_to_Production.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Encoder-Decoder Transformer\n",
        "We have seen Encoder only and Decoder only Transformers in the last weeks and today we take a look at the orginal proposed architecture that involves both elements. It is perfect for Sequence to Sequence models like Translation tasks.\n",
        "\n",
        "After taking a look at the Data-Flow and how Encoder and Decoder interact, we train a summarization model that can summerize Reddit posts into shorter \"too long didn't read\" versions."
      ],
      "metadata": {
        "id": "l4k4vqWkiK0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Encoder Decoder Data-Flow\n",
        "\n",
        "This time we show you a few time steps of a translation. We translate \"I love you!\" to the German equivalent \"Ich liebe dich!\". We just drop the last step in which we predict a \"!\"."
      ],
      "metadata": {
        "id": "TG5hLNnvjt_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "NpyZasVOPj8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "# This cell installs all the dependencies used in this week - imports happen as we go\n",
        "%%capture\n",
        "!pip install transformers==4.24.0 datasets evaluate rouge_score nltk optimum[onnxruntime]\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "PfrlFKYg5JjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Idea To Production - A Capstone Project "
      ],
      "metadata": {
        "id": "Fw6xNLuAEYnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Your own Reddit Summarization Service"
      ],
      "metadata": {
        "id": "7cajGgOvEgPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to wrap it all up - train an Endcoder-Decoder Transformer (a BART model from Meta) and optimize it for production. After that we will apply all the knowledge we have about embeddings to identify out-of-distribution behaviour e.g. when our model recieves input or generates texts we do not want it to process and or produce."
      ],
      "metadata": {
        "id": "6r8GJPJzEbb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might already be familiar with loading datasets fom Huggingface. This is a dataset of Reddit posts where authors decided to leave a \"TL;DR\" (too long didn't read) summary for impatient readers. Since the main post and the summary have a different writing style this is a Sequence to Sequence task - perfect for the Encoder-Decoder Transformer. Using the \"most complex\" version of the standard Transformer architecture also comes with the benefit of more effective context length since the input text does not share space with the output text."
      ],
      "metadata": {
        "id": "DhEAKzLJ6Qrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ Reddit is an online community that had and has some unristricted content "
      ],
      "metadata": {
        "id": "tOvKtKhfCE06"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H14X5smlET3o"
      },
      "outputs": [],
      "source": [
        "# You know the drill, streaming=True and sample!\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "reddit = load_dataset(\"reddit\", split=\"train\", streaming=True)\n",
        "shuffled_reddit = reddit.shuffle(seed=42, buffer_size=2000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Again, we can fetch a few samples through itertools\n",
        "import itertools\n",
        "\n",
        "N = 200\n",
        "reddit_sample = list(itertools.islice(iter(shuffled_reddit), N))"
      ],
      "metadata": {
        "id": "ofDeNH4q6j2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data is more easy to explore in Pandas\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(reddit_sample)"
      ],
      "metadata": {
        "id": "TBdGZsmW6nGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are most interested in the content and summary columns\n",
        "df"
      ],
      "metadata": {
        "id": "5cmAVCMk62IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As introduced in last session we can let Huggingface do the train-test split and create a DatasetDict."
      ],
      "metadata": {
        "id": "SN8I8HwU7bu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "ds = Dataset.from_pandas(df).train_test_split(test_size=0.15)"
      ],
      "metadata": {
        "id": "BbIdQG4u7muV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initialise the BART model in the usual way. This time we explicitly load a configuration prepared by the people who pretrained the model."
      ],
      "metadata": {
        "id": "4XjQoxKm7azp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "config = AutoConfig.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "# Bart uses a byte-level Byte-Pair-Encoding - it can treat spaces as part of words\n",
        "# The default for this behaviour is: turned off\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"facebook/bart-base\",\n",
        "    config=config\n",
        ")"
      ],
      "metadata": {
        "id": "LmDRzzZZ7ru8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we define a preprocessing function. This time it has to process inputs of the Encoder and Decoder and the desired Decoder output we calculate our loss on is called \"labels\". To incentive the pretrained model to generate a summarization we us the \"summarize\" prompt."
      ],
      "metadata": {
        "id": "nF-A5FLE8vqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_column = \"content\"\n",
        "summary_column = \"summary\"\n",
        "prefix = \"summarize: \"\n",
        "\n",
        "padding = \"max_length\"\n",
        "max_source_length = 1024 # as mentioned above we have this space for input and output each\n",
        "max_target_length = 128  # ... but we force the summaries to be \"short\"\n",
        "ignore_pad_token_for_loss=True\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs, targets = [], []\n",
        "\n",
        "    # Unpack the examples\n",
        "    for i in range(len(examples[text_column])):\n",
        "        if examples[text_column][i] and examples[summary_column][i]:\n",
        "            inputs.append(examples[text_column][i])     # This goes into the Encoder\n",
        "            targets.append(examples[summary_column][i]) # This goes into the Decoder\n",
        "\n",
        "    # Tokenize Encoder Inputs\n",
        "    inputs = [prefix + inp for inp in inputs]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Tokenize Decoder Inputs\n",
        "    labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Optionally, apply padding\n",
        "    if padding == \"max_length\" and ignore_pad_token_for_loss:\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "\n",
        "    # The tokenizer results are dicts, we can easily add the labels as a new property\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "pT38P0dq8tvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before Preprocessing\n",
        "ds"
      ],
      "metadata": {
        "id": "Hh2IDQVR-f5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we do not remove all columns not needed for the model, the Data Collator will blindly tokenize everything and the model might crash where dicts are unpacked as kwargs via **dict because of unexpected kwargs. Amazing!"
      ],
      "metadata": {
        "id": "hechYLi9-JKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_ds = ds.map(preprocess_function, batched=True, remove_columns=ds[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "XRF2gthM94cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After Preprocessing\n",
        "tokenized_ds"
      ],
      "metadata": {
        "id": "QJGuyZc_-EuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, there is a specialised Data Collator for jsut this task."
      ],
      "metadata": {
        "id": "GQYATbLb-dNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=tokenizer.pad_token_id\n",
        "    )\n"
      ],
      "metadata": {
        "id": "2UWbg6i7-4Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Huggingface Metrics\n",
        "\n",
        "Huggingface hosts scripts to calculate the metrics used in publiscations to train models like BERT, Bart and so on."
      ],
      "metadata": {
        "id": "uMvTdNy4-iuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROUGE** (Recall-Oriented Understudy for Gisting Evaluation) is family of metrics published by Google in 2004 to measure translation tasks. In 2017 they added a summary oriented metric: *rougeLSum*"
      ],
      "metadata": {
        "id": "0Y4qtUTB_DrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The evaluate package has been installed in the setup cell at the top of this notebook\n",
        "import evaluate\n",
        "\n",
        "\n",
        "metric = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "YP54R40__IRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this task we need a postprocessing function that alters the output to fit the parameters of the *rougeLSum* metric."
      ],
      "metadata": {
        "id": "AmpDnILV_86J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Did you expect us impoting NLTK ever again! Surprise!\n",
        "import nltk\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "\n",
        "    # rougeLSum expects a newline after each sentence\n",
        "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "    return preds, labels"
      ],
      "metadata": {
        "id": "94gw3mh7AREm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Huggingface Trainer can work with any metric passed as the \"compute_metrics\" parameter. You will often see that people call the function to be called jsut as the parameter."
      ],
      "metadata": {
        "id": "lepNoIJYAXWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    if True:\n",
        "        # Replace -100 in the labels as we can't decode them.\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "ORPay32a_Gzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a Sequence-to-Sequence model"
      ],
      "metadata": {
        "id": "yCkpuIYIEVto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully, this shows the power of the Huggingface Trainer. There is so much to keep an eye on during this training, losses, metrics, different spaces... As last week, we do not have enough time to let you train a full model during this session."
      ],
      "metadata": {
        "id": "ZqIjKGntAwn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Initialize our Trainer\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"bart-reddit\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=6,\n",
        "    logging_steps=6,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.1,\n",
        "    warmup_steps=1_000,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=5e-4,\n",
        "    save_steps=6,\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=4,\n",
        "    generation_max_length=max_target_length,\n",
        "    predict_with_generate=True\n",
        "    #fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "t95dg4xD_kuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's go!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "cA53MAOT_52f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try the results of this mini trainig\n",
        "# First, we tokenize a post\n",
        "example = next(iter(shuffled_reddit))\n",
        "\n",
        "tokenized_content = tokenizer(example[\"content\"], return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "# Then, we let the model run multiple times though the generate method until \n",
        "# it encoutners a <|endoftext|> or reaches a deined max_length\n",
        "generated = model.generate(input_ids=tokenized_content[\"input_ids\"].cuda(), \n",
        "                            attention_mask=tokenized_content[\"attention_mask\"].cuda(),\n",
        "                            temperature=0.9,\n",
        "                            early_stopping=True,\n",
        "                            do_sample=True,\n",
        "                            max_new_tokens=max_target_length)"
      ],
      "metadata": {
        "id": "2eG7EaXSBQD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we just have to decode the tokens to characters\n",
        "tokenizer.batch_decode(generated, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "s9Ybu5KNDx7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And this is the \"truth\"\n",
        "example[\"summary\"]"
      ],
      "metadata": {
        "id": "0HL45BjLEBsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fast in Production - Quantization\n",
        "\n",
        "You probably want to run these large models as fast as possible in production, but they come with high resource needs. First, let's compare using GPU or CPU for inference and than move on to optimization for CPU via Quantization."
      ],
      "metadata": {
        "id": "5RUXosxrBNkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need a single test observation and a batch test - the tokenizer is super fast and not part of this comparision\n",
        "\n",
        "single = tokenized_content\n",
        "\n",
        "batch = {\n",
        "    \"input_ids\": tokenized_content[\"input_ids\"].repeat(5, 1),\n",
        "    \"attention_mask\": tokenized_content[\"attention_mask\"].repeat(5, 1)\n",
        "}"
      ],
      "metadata": {
        "id": "bSPkw-vqGW90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU - Single"
      ],
      "metadata": {
        "id": "piXNN9UPHgts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 5\n",
        "model.generate(input_ids=single[\"input_ids\"].cuda(), \n",
        "                            attention_mask=single[\"attention_mask\"].cuda(),\n",
        "                            temperature=0.9,\n",
        "                            early_stopping=True,\n",
        "                            do_sample=True,\n",
        "                            max_new_tokens=max_target_length)"
      ],
      "metadata": {
        "id": "tjZtaixDHmxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU - Batch"
      ],
      "metadata": {
        "id": "5xvUilaVHrtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 5\n",
        "model.generate(input_ids=batch[\"input_ids\"].cuda(), \n",
        "                            attention_mask=batch[\"attention_mask\"].cuda(),\n",
        "                            temperature=0.9,\n",
        "                            early_stopping=True,\n",
        "                            do_sample=True,\n",
        "                            max_new_tokens=max_target_length)"
      ],
      "metadata": {
        "id": "y5QnUEM6HuM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As long as there is enough RAM a GPU parallelizes perfectly.\n",
        "\n",
        "To test the CPU performance, we haev to move our model to CPU."
      ],
      "metadata": {
        "id": "23lEqxBxISEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to('cpu')"
      ],
      "metadata": {
        "id": "6kyif6vRIjSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CPU - Single"
      ],
      "metadata": {
        "id": "gdrzHVpcIMOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 5\n",
        "model.generate(input_ids=single[\"input_ids\"], \n",
        "                            attention_mask=single[\"attention_mask\"],\n",
        "                            temperature=0.9,\n",
        "                            early_stopping=True,\n",
        "                            do_sample=True,\n",
        "                            max_new_tokens=max_target_length)"
      ],
      "metadata": {
        "id": "XQT8ZoSFIOOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CPU - Batch"
      ],
      "metadata": {
        "id": "zkCtw0tEIOtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 5\n",
        "model.generate(input_ids=batch[\"input_ids\"], \n",
        "                            attention_mask=batch[\"attention_mask\"],\n",
        "                            temperature=0.9,\n",
        "                            early_stopping=True,\n",
        "                            do_sample=True,\n",
        "                            max_new_tokens=max_target_length)"
      ],
      "metadata": {
        "id": "KotO4rffEt69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far so good, CPU is way slower and in batch mode it does not parallelize well. But GPUs are expensive and you might not have them where the models need to run."
      ],
      "metadata": {
        "id": "lFXhP6EFIyBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Quantization via Optimum\n",
        "\n",
        "To use Huggingface's implementation adapter of the Microsoft ONNX Runtime we have to save our model to disk. It's a good practice to add the matching Tokenizer to the same directory."
      ],
      "metadata": {
        "id": "uhyKTh3pKj3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"amazing_summarization_model\")\n",
        "tokenizer.save_pretrained(\"amazing_summarization_model\")"
      ],
      "metadata": {
        "id": "HsDi9a-ZJv5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U tensorflow==2.10 "
      ],
      "metadata": {
        "id": "W7P2WeWCXOk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
        "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
        "\n",
        "quantization_model = ORTModelForSeq2SeqLM.from_pretrained(\"./amazing_summarization_model\", from_transformers=True)"
      ],
      "metadata": {
        "id": "9WQdofhUEt4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimized - Single"
      ],
      "metadata": {
        "id": "Neva2kBCORlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 5\n",
        "quantization_model.generate(input_ids=single[\"input_ids\"], \n",
        "                            attention_mask=single[\"attention_mask\"],\n",
        "                            temperature=0.9,\n",
        "                            early_stopping=True,\n",
        "                            do_sample=True,\n",
        "                            max_new_tokens=max_target_length)"
      ],
      "metadata": {
        "id": "3IvO5u6_EtzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimized - Batch"
      ],
      "metadata": {
        "id": "G69x_M4aOUjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 5\n",
        "quantization_model.generate(input_ids=batch[\"input_ids\"], \n",
        "                            attention_mask=batch[\"attention_mask\"],\n",
        "                            temperature=0.9,\n",
        "                            early_stopping=True,\n",
        "                            do_sample=True,\n",
        "                            max_new_tokens=max_target_length)"
      ],
      "metadata": {
        "id": "dpRxVR3JOYC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, Quantization reduced the compute time for single predictions, but hardly helped with Batches. As a rule of thumb: Do not use CPUs for batch predictions."
      ],
      "metadata": {
        "id": "q0zKruzKObdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up out limited memory\n",
        "del quantization_model\n",
        "del ds\n",
        "del tokenized_ds"
      ],
      "metadata": {
        "id": "rOrgbvb8Q6Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting Out of distribution Inputs and Putputs\n",
        "In this final section of the course we want to identify potential abuse of our models by identifing inputs we should not predict on - the same techinque can be used on outputs as well. This area of deep learning research is still in its infancy and there are no \"one-size-fits-all-solutions\" yet. Huggingface currently does not include any tooling to do so."
      ],
      "metadata": {
        "id": "Km7lOPX7Ghgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In week 3 we built a search engine that allowed search through similarity in the latent space. Well, we can use distance in latent space to define \"too different from what we noramlly do\", too. There are many ways to do so - one could use attention similarity, CLS token similarity, mean embedding similarity and so on...\n",
        "\n",
        "For this example we will make use of the fact that Reddit consists of different communities called Subreddits that discuss \"different\" topics. We can setup an OOD example by pretending that some subreddits contain \"unwelcome\" content.\n",
        "\n",
        "For the sake of time we do not use the entire dataset here, again."
      ],
      "metadata": {
        "id": "E-Og0WcE9kXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The full reddit are 2000 posts - take it or leave it :P\n",
        "reddit_full = list(itertools.islice(iter(shuffled_reddit), 2000))"
      ],
      "metadata": {
        "id": "wXh4TGy8Gh5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_full = pd.DataFrame(reddit_full)\n",
        "df_full"
      ],
      "metadata": {
        "id": "rqJMzxNEr0Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's remember that all these models are doing is basically finding space transformations - and for checking undesired inputs we would choose the last hidden state of the encoder."
      ],
      "metadata": {
        "id": "7l7Ls6ANsVix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_embed_in_latent_space(text, model=model, tokenizer=tokenizer):\n",
        "    tokenized_text = tokenizer(text, return_tensors=\"pt\", max_length=1024, padding=True, truncation=True)\n",
        "\n",
        "    # We set output_hidden_states to True to return encoder and decoder hidden states\n",
        "    outputs = model(input_ids=tokenized_text[\"input_ids\"].cuda(), attention_mask=tokenized_text[\"attention_mask\"].cuda(), output_hidden_states=True)\n",
        "\n",
        "    # But eventually we are only interested in the encoder_last_hidden_state\n",
        "    return outputs.encoder_last_hidden_state.detach().cpu().numpy()[:,0]  #.mean(axis=1)"
      ],
      "metadata": {
        "id": "dgKyX1h-sLpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks similar to building an Annoy index and indeed it is very similar - we embed all the examples and calculate a mean per subreddit. This is a very simplistic method, but quite effective."
      ],
      "metadata": {
        "id": "K29hd4Dd_mIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "\n",
        "batch_size = 8\n",
        "n_batches = N // batch_size\n",
        "f = 768  # Length of item vector that will be indexed\n",
        "unique_id = 0\n",
        "\n",
        "subreddit_spaces = defaultdict(list)\n",
        "\n",
        "# Move model back to GPU\n",
        "model.cuda()\n",
        "\n",
        "for i in tqdm(range(n_batches)):\n",
        "    batch = [post for post in df_full.to_dict('records')[i * batch_size : (1 + i) * batch_size]]\n",
        "    with torch.no_grad():\n",
        "        batched_output = batch_embed_in_latent_space([post[\"content\"] for post in batch], model=model, tokenizer=tokenizer)\n",
        "\n",
        "    for vector, post in zip(batched_output, batch):\n",
        "        subreddit_spaces[post[\"subreddit\"]].append(vector)"
      ],
      "metadata": {
        "id": "Gm3kC8MYtUXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we calculate the mean 768 embedding dimensions per subreddit\n",
        "mean_subreddit_spaces = {}\n",
        "\n",
        "for subreddit, vectors in subreddit_spaces.items():\n",
        "    mean_subreddit_spaces[subreddit] = np.mean(vectors, axis=0)"
      ],
      "metadata": {
        "id": "NihqdM4tuIww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, having a mean representation of each subreddit we can calculate their pairwise similarity to define which subreddits are \"hostile\". For an in production \"filter\" we could instead create a Gaussian Mixture Model."
      ],
      "metadata": {
        "id": "Jqv8ytGDABQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "n = len(mean_subreddit_spaces.values())\n",
        "a = np.zeros((n, n))\n",
        "\n",
        "values = list(mean_subreddit_spaces.values())\n",
        "\n",
        "for y in range(n):\n",
        "    for x in range(n):\n",
        "        a[y][x] = cosine_similarity(values[y].reshape(1, -1), values[x].reshape(1, -1))"
      ],
      "metadata": {
        "id": "81SCkB_P61b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual inspection shows that therer are very similar and less similar subreddits - good!\n",
        "import seaborn as sns\n",
        "\n",
        "sns.heatmap(a)"
      ],
      "metadata": {
        "id": "BsmsUmAh6Tqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a histogram of similarieties - you could have one for incoming requests in production."
      ],
      "metadata": {
        "id": "qwJxkLsBBe-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarities = []\n",
        "\n",
        "for y in range(n):\n",
        "    for x in range(n - y, n):\n",
        "        similarities.append(a[y][x])"
      ],
      "metadata": {
        "id": "n-unSre99y7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(similarities).plot.hist()"
      ],
      "metadata": {
        "id": "CHj6J1QcE7uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find the most unsimilar Subreddits"
      ],
      "metadata": {
        "id": "xrKTM1xSCKtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_i, x_i = np.unravel_index(a.argmin(), a.shape)"
      ],
      "metadata": {
        "id": "5Xj688We6gjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(mean_subreddit_spaces.keys())[y_i], list(mean_subreddit_spaces.keys())[x_i], a[y_i][x_i]"
      ],
      "metadata": {
        "id": "LvGTlSee7z52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the most similar subreddits"
      ],
      "metadata": {
        "id": "AaDmMR31CVYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_index_pairs = np.unravel_index(np.argsort(a, axis=None), a.shape)"
      ],
      "metadata": {
        "id": "S3so6J6iC-wV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_i, x_i = sorted_index_pairs[0][-n -1], sorted_index_pairs[1][-n -1]"
      ],
      "metadata": {
        "id": "tOOgna3rDrI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(mean_subreddit_spaces.keys())[y_i], list(mean_subreddit_spaces.keys())[x_i], a[y_i][x_i]"
      ],
      "metadata": {
        "id": "XQtGi_6UD8rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, we could define a threshold of similarity for inputs and inputs falling below it would not be processed. Similarly, we could check the emebddings of our outputs and cancel \"risky\" outputs before they are sent as a response."
      ],
      "metadata": {
        "id": "14dY58yHDHky"
      }
    }
  ]
}